import Anthropic from '@anthropic-ai/sdk'
import OpenAI from 'openai'
import { GoogleGenerativeAI } from '@google/generative-ai'
import { MCPCache } from './mcp-cache'

export type AIModel = 'gemini' | 'claude' | 'gpt' | 'ollama' | 'auto'
export type ExamType = 'TYT' | 'AYT'

export interface ModelConfig {
  model: AIModel
  priority: number
  enabled: boolean
  apiKey?: string
  maxRetries: number
  timeout: number
}

export interface SolveRequest {
  image: string // base64
  mimeType: string
  subject: string
  examType: ExamType
  preferredModel?: AIModel
  requireMultipleModels?: boolean
}

export interface ModelResponse {
  model: AIModel
  solution: string
  confidence: number
  processingTime: number
  error?: string
}

export class MCPService {
  private models: Map<AIModel, ModelConfig>
  private anthropic?: Anthropic
  private openai?: OpenAI
  private gemini?: GoogleGenerativeAI
  private cache: MCPCache

  constructor() {
    this.models = new Map()
    this.cache = new MCPCache(500, 12) 
    this.initializeModels()
  }

  private initializeModels() {
    if (process.env.GEMINI_API_KEY) {
      this.gemini = new GoogleGenerativeAI(process.env.GEMINI_API_KEY)
      this.models.set('gemini', {
        model: 'gemini',
        priority: 1,
        enabled: true,
        maxRetries: 3,
        timeout: 30000
      })
    }

    if (process.env.ANTHROPIC_API_KEY) {
      this.anthropic = new Anthropic({
        apiKey: process.env.ANTHROPIC_API_KEY
      })
      this.models.set('claude', {
        model: 'claude',
        priority: 2,
        enabled: true,
        maxRetries: 3,
        timeout: 45000
      })
    }

    if (process.env.OPENAI_API_KEY) {
      this.openai = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY
      })
      this.models.set('gpt', {
        model: 'gpt',
        priority: 3,
        enabled: true,
        maxRetries: 3,
        timeout: 40000
      })
    }

    if (process.env.OLLAMA_ENABLED === 'true') {
      this.models.set('ollama', {
        model: 'ollama',
        priority: 4,
        enabled: true,
        maxRetries: 2,
        timeout: 60000
      })
    }
  }

  async solve(request: SolveRequest): Promise<ModelResponse[]> {
    const { preferredModel, requireMultipleModels } = request

    const cacheKey = this.cache.generateKey(
      request.image.substring(0, 100), 
      request.subject, 
      request.examType
    )
    
    if (!requireMultipleModels && process.env.CACHE_ENABLED !== 'false') {
      const cachedSolution = this.cache.get(cacheKey)
      if (cachedSolution) {
        return [{
          model: preferredModel || 'gemini',
          solution: cachedSolution,
          confidence: 0.95,
          processingTime: 10,
          error: undefined
        }]
      }
    }

    let responses: ModelResponse[]

    if (preferredModel && !requireMultipleModels) {
      const response = await this.solveWithModel(preferredModel, request)
      responses = [response]
    }
    else if (requireMultipleModels) {
      responses = await this.solveWithMultipleModels(request)
    }
    else {
      responses = await this.solveWithAutoSelect(request)
    }

    if (responses.length > 0 && responses[0].solution && !responses[0].error) {
      this.cache.set(cacheKey, responses[0].solution, responses[0].model)
    }

    return responses
  }

  private async solveWithModel(model: AIModel, request: SolveRequest): Promise<ModelResponse> {
    const startTime = Date.now()

    try {
      let solution = ''

      switch (model) {
        case 'gemini':
          solution = await this.solveWithGemini(request)
          break
        case 'claude':
          solution = await this.solveWithClaude(request)
          break
        case 'gpt':
          solution = await this.solveWithGPT(request)
          break
        case 'ollama':
          solution = await this.solveWithOllama(request)
          break
        default:
          throw new Error(`Desteklenmeyen model: ${model}`)
      }

      return {
        model,
        solution,
        confidence: this.calculateConfidence(solution),
        processingTime: Date.now() - startTime
      }
    } catch (error) {
      return {
        model,
        solution: '',
        confidence: 0,
        processingTime: Date.now() - startTime,
        error: error instanceof Error ? error.message : 'Bilinmeyen hata'
      }
    }
  }

  private async solveWithGemini(request: SolveRequest): Promise<string> {
    if (!this.gemini) throw new Error('Gemini API yapÄ±landÄ±rÄ±lmamÄ±ÅŸ')

    const model = this.gemini.getGenerativeModel({ model: "gemini-1.5-flash" })
    const prompt = this.generatePrompt(request.subject, request.examType, 'gemini')

    const result = await model.generateContent([
      prompt,
      {
        inlineData: {
          data: request.image,
          mimeType: request.mimeType,
        },
      },
    ])

    return result.response.text() || 'Ã‡Ã¶zÃ¼m oluÅŸturulamadÄ±.'
  }

  private async solveWithClaude(request: SolveRequest): Promise<string> {
    if (!this.anthropic) throw new Error('Claude API yapÄ±landÄ±rÄ±lmamÄ±ÅŸ')

    const prompt = this.generatePrompt(request.subject, request.examType, 'claude')

    const response = await this.anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 4096,
      messages: [{
        role: 'user',
        content: [
          {
            type: 'text',
            text: prompt
          },
          {
            type: 'image',
            source: {
              type: 'base64',
              media_type: request.mimeType as any,
              data: request.image
            }
          }
        ]
      }]
    })

    return response.content[0].type === 'text' ? response.content[0].text : 'Ã‡Ã¶zÃ¼m oluÅŸturulamadÄ±.'
  }

  private async solveWithGPT(request: SolveRequest): Promise<string> {
    if (!this.openai) throw new Error('OpenAI API yapÄ±landÄ±rÄ±lmamÄ±ÅŸ')

    const prompt = this.generatePrompt(request.subject, request.examType, 'gpt')

    const response = await this.openai.chat.completions.create({
      model: 'gpt-4-vision-preview',
      messages: [{
        role: 'user',
        content: [
          { type: 'text', text: prompt },
          {
            type: 'image_url',
            image_url: {
              url: `data:${request.mimeType};base64,${request.image}`
            }
          }
        ]
      }],
      max_tokens: 4096
    })

    return response.choices[0]?.message?.content || 'Ã‡Ã¶zÃ¼m oluÅŸturulamadÄ±.'
  }

  private async solveWithOllama(request: SolveRequest): Promise<string> {
    const ollamaHost = process.env.OLLAMA_HOST || 'http://127.0.0.1:11434'
    
    try {
      const modelsResponse = await fetch(`${ollamaHost}/api/tags`)
      if (!modelsResponse.ok) throw new Error('Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±sÄ±z')
      
      const modelsData = await modelsResponse.json()
      const availableModels = modelsData.models?.map((m: any) => m.name) || []
      
      let modelName = 'phi4-mini:latest' // VarsayÄ±lan
      if (availableModels.includes('llava:latest')) {
        modelName = 'llava:latest'
      } else if (availableModels.includes('llava')) {
        modelName = 'llava'
      } else if (availableModels.length > 0) {
        modelName = availableModels[0]
      }


      const isVisionModel = modelName.includes('llava')
      
      const requestBody: any = {
        model: modelName,
        prompt: this.generatePrompt(request.subject, request.examType, 'ollama'),
        stream: false
      }

      if (isVisionModel) {
        requestBody.images = [request.image]
      } else {
        requestBody.prompt += '\n\nNot: Bu bir gÃ¶rsel soru Ã§Ã¶zÃ¼mÃ¼ isteÄŸidir. GÃ¶rsel iÃ§eriÄŸi analiz edemediÄŸim iÃ§in genel bir yaklaÅŸÄ±m sunacaÄŸÄ±m.'
      }

      const response = await fetch(`${ollamaHost}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(requestBody)
      })

      if (!response.ok) {
        throw new Error(`Ollama API hatasÄ±: ${response.status} ${response.statusText}`)
      }

      const data = await response.json()
      return data.response || 'Ã‡Ã¶zÃ¼m oluÅŸturulamadÄ±.'
      
    } catch (error) {
      console.error('Ollama error:', error)
      throw new Error(`Ollama baÄŸlantÄ± hatasÄ±: ${error instanceof Error ? error.message : 'Bilinmeyen hata'}`)
    }
  }

  private async solveWithMultipleModels(request: SolveRequest): Promise<ModelResponse[]> {
    const enabledModels = Array.from(this.models.entries())
      .filter(([_, config]) => config.enabled)
      .sort((a, b) => a[1].priority - b[1].priority)

    const promises = enabledModels.map(([model]) => 
      this.solveWithModel(model, request)
    )

    const responses = await Promise.allSettled(promises)
    
    return responses
      .filter((r): r is PromiseFulfilledResult<ModelResponse> => r.status === 'fulfilled')
      .map(r => r.value)
      .filter(r => r.confidence > 0.3) // DÃ¼ÅŸÃ¼k gÃ¼venli Ã§Ã¶zÃ¼mleri filtrele
  }

  private async solveWithAutoSelect(request: SolveRequest): Promise<ModelResponse[]> {
    const enabledModels = Array.from(this.models.entries())
      .filter(([_, config]) => config.enabled)
      .sort((a, b) => a[1].priority - b[1].priority)

    for (const [model] of enabledModels) {
      const response = await this.solveWithModel(model, request)
      if (response.confidence > 0.7 && !response.error) {
        return [response]
      }
    }

    const allResponses = await this.solveWithMultipleModels(request)
    return allResponses.slice(0, 1)
  }

  private generatePrompt(subject: string, examType: ExamType, model: AIModel): string {
    const basePrompt = examType === 'TYT' 
      ? this.generateTYTPrompt(subject) 
      : this.generateAYTPrompt(subject)

    const modelOptimizations: Record<AIModel, string> = {
      gemini: '\n\nGÃ¶rsel analiz ve hÄ±zlÄ± Ã§Ã¶zÃ¼m odaklÄ± yaklaÅŸ.',
      claude: '\n\nDetaylÄ± mantÄ±ksal aÃ§Ä±klamalar ve adÄ±m adÄ±m analiz yap.',
      gpt: '\n\nYaratÄ±cÄ± Ã§Ã¶zÃ¼m yÃ¶ntemleri ve alternatif yaklaÅŸÄ±mlar sun.',
      ollama: '\n\nTemel kavramlara odaklan ve basit aÃ§Ä±klamalar kullan.',
      auto: ''
    }

    return basePrompt + modelOptimizations[model]
  }

  private generateTYTPrompt(subject: string): string {
    return `Sen uzman bir TYT ${subject} Ã¶ÄŸretmenisin. GÃ¶rseldeki TYT ${subject} sorusunu Ã§Ã¶z.

Ã‡Ã¶zÃ¼mde ÅŸunlara dikkat et:
1. TYT seviyesine uygun temel kavramlarÄ± kullan
2. AdÄ±m adÄ±m, anlaÅŸÄ±lÄ±r ÅŸekilde aÃ§Ä±kla
3. Gereksiz detaylara girme
4. Pratik Ã§Ã¶zÃ¼m yÃ¶ntemlerini gÃ¶ster
5. Benzer soru tipleri iÃ§in ipuÃ§larÄ± ver

Ã‡Ã¶zÃ¼mÃ¼ ÅŸu formatta sun:
ğŸ“ **Soru Analizi**
ğŸ”‘ **Temel Kavramlar**
ğŸ”„ **Ã‡Ã¶zÃ¼m AdÄ±mlarÄ±**
âœ… **SonuÃ§**
ğŸ’¡ **Ä°puÃ§larÄ±**

TÃ¼rkÃ§e olarak cevap ver ve aÃ§Ä±klamalarÄ±nÄ± mÃ¼mkÃ¼n olduÄŸunca detaylÄ± yap.`
  }

  private generateAYTPrompt(subject: string): string {
    return `Sen uzman bir AYT ${subject} Ã¶ÄŸretmenisin. GÃ¶rseldeki AYT ${subject} sorusunu Ã§Ã¶z.

Ã‡Ã¶zÃ¼mde ÅŸunlara dikkat et:
1. AYT seviyesine uygun ileri dÃ¼zey analiz yap
2. DetaylÄ± ve kapsamlÄ± aÃ§Ä±kla
3. FarklÄ± Ã§Ã¶zÃ¼m yÃ¶ntemlerini gÃ¶ster
4. Konunun diÄŸer konularla iliÅŸkisini kur
5. Ãœniversite sÄ±navÄ± odaklÄ± stratejiler sun

Ã‡Ã¶zÃ¼mÃ¼ ÅŸu formatta sun:
ğŸ“Š **DetaylÄ± Soru Analizi**
ğŸ¯ **Ä°leri DÃ¼zey Kavramlar**
ğŸ”¬ **Ã‡Ã¶zÃ¼m Metodolojisi**
ğŸ”€ **Alternatif Ã‡Ã¶zÃ¼m YollarÄ±**
ğŸ“ˆ **SonuÃ§ ve DeÄŸerlendirme**
âš¡ **AYT Stratejileri**

TÃ¼rkÃ§e olarak cevap ver ve aÃ§Ä±klamalarÄ±nÄ± mÃ¼mkÃ¼n olduÄŸunca detaylÄ± yap.`
  }

  private calculateConfidence(solution: string): number {
    if (!solution || solution.length < 100) return 0

    let score = 0.5 

    if (solution.includes('AdÄ±m') || solution.includes('adÄ±m')) score += 0.1
    if (solution.includes('SonuÃ§') || solution.includes('sonuÃ§')) score += 0.1
    if (solution.includes('**') || solution.includes('##')) score += 0.1 
    if (solution.length > 500) score += 0.1
    if (solution.includes('FormÃ¼l') || solution.includes('formÃ¼l')) score += 0.1

    return Math.min(score, 1)
  }

  getModelStatus(): Record<AIModel, boolean> {
    const status: Record<AIModel, boolean> = {
      gemini: false,
      claude: false,
      gpt: false,
      ollama: false,
      auto: true
    }

    this.models.forEach((config, model) => {
      status[model] = config.enabled
    })

    return status
  }
}